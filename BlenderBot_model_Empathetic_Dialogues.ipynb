{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1bba7aa",
   "metadata": {},
   "source": [
    "### Model 1: BlenderBot model on the Empathetic Dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e544deb-db5a-4c7e-b168-22ebf3608828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    BlenderbotTokenizer,\n",
    "    BlenderbotForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer\n",
    ")\n",
    "from datasets import DatasetDict, Dataset, load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f33c9c7b-ed1f-4bbf-97ae-9904cbf9eeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b48a38-affc-4060-bb87-e86863144788",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1e41522-718c-437f-982c-0e5c42ae0f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"facebook/empathetic_dialogues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7d96559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"empathetic_dialogues\")\n",
    "train_dataset = Dataset.from_file(\"./empathetic_dialogues-train.arrow\")\n",
    "\n",
    "val_dataset = Dataset.from_file(\"./empathetic_dialogues-validation.arrow\")\n",
    "\n",
    "test_dataset = Dataset.from_file(\"./empathetic_dialogues-test.arrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8a942e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e68b8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance', 'selfeval', 'tags'],\n",
       "    num_rows: 76673\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3b100f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv_id': ['hit:0_conv:1',\n",
       "  'hit:0_conv:1',\n",
       "  'hit:0_conv:1',\n",
       "  'hit:0_conv:1',\n",
       "  'hit:0_conv:1'],\n",
       " 'utterance_idx': [1, 2, 3, 4, 5],\n",
       " 'context': ['sentimental',\n",
       "  'sentimental',\n",
       "  'sentimental',\n",
       "  'sentimental',\n",
       "  'sentimental'],\n",
       " 'prompt': ['I remember going to the fireworks with my best friend. There was a lot of people_comma_ but it only felt like us in the world.',\n",
       "  'I remember going to the fireworks with my best friend. There was a lot of people_comma_ but it only felt like us in the world.',\n",
       "  'I remember going to the fireworks with my best friend. There was a lot of people_comma_ but it only felt like us in the world.',\n",
       "  'I remember going to the fireworks with my best friend. There was a lot of people_comma_ but it only felt like us in the world.',\n",
       "  'I remember going to the fireworks with my best friend. There was a lot of people_comma_ but it only felt like us in the world.'],\n",
       " 'speaker_idx': [1, 0, 1, 0, 1],\n",
       " 'utterance': ['I remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people_comma_ we felt like the only people in the world.',\n",
       "  'Was this a friend you were in love with_comma_ or just a best friend?',\n",
       "  'This was a best friend. I miss her.',\n",
       "  'Where has she gone?',\n",
       "  'We no longer talk.'],\n",
       " 'selfeval': ['5|5|5_2|2|5',\n",
       "  '5|5|5_2|2|5',\n",
       "  '5|5|5_2|2|5',\n",
       "  '5|5|5_2|2|5',\n",
       "  '5|5|5_2|2|5'],\n",
       " 'tags': ['', '', '', '', '']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1681fb5b-5126-4a08-8aef-f4326a5f5ed3",
   "metadata": {},
   "source": [
    "### Preprocess dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd78b14-964c-437f-a54c-d7f7956facdb",
   "metadata": {},
   "source": [
    "Here, we process the dialogue dataset by cleaning the text and constructing contextual input-response pairs (tagged with emotions) based on speaker turns. This helps create a realistic conversational setup for training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca0799f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_empathetic_dataset(dataset_split, max_turns=4):\n",
    "    def clean_text(text):\n",
    "        replacements = {'_comma_': ',', '_period_': '.', '_exclamation_': '!'}\n",
    "        for k, v in replacements.items():\n",
    "            text = text.replace(k, v)\n",
    "        return text.strip()\n",
    "\n",
    "    df = pd.DataFrame(dataset_split)\n",
    "    df = df.sort_values(by=['conv_id', 'utterance_idx']).reset_index(drop=True)\n",
    "\n",
    "    pairs = []\n",
    "    for conv_id, conv in df.groupby('conv_id'):\n",
    "        history = []  # store (speaker, utterance)\n",
    "        for i, row in conv.iterrows():\n",
    "            utterance = clean_text(row['utterance'])\n",
    "            speaker = row['speaker_idx']\n",
    "            emotion = row['context']\n",
    "\n",
    "            # Only create a pair if there's history and speaker has changed\n",
    "            if history and speaker != history[-1][0]:\n",
    "                # Extract only the utterances from history\n",
    "                context_utts = [utt for _, utt in history[-max_turns:]]\n",
    "                context = \" [SEP] \".join(context_utts)\n",
    "                input_text = f\"<emotion={emotion}> [CONTEXT] {context} [USER]\"\n",
    "                \n",
    "                pairs.append({\n",
    "                    \"input_text\": input_text,\n",
    "                    \"response\": utterance,\n",
    "                    \"emotion\": emotion\n",
    "                })\n",
    "\n",
    "            history.append((speaker, utterance))\n",
    "\n",
    "    return Dataset.from_pandas(pd.DataFrame(pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a24a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to each split\n",
    "dataset_processed = DatasetDict({\n",
    "    \"train\": process_empathetic_dataset(dataset['train']),\n",
    "    \"validation\": process_empathetic_dataset(dataset['validation']),\n",
    "    \"test\": process_empathetic_dataset(dataset['test'])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26e774b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_text': ['<emotion=sentimental> [CONTEXT] I remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world. [USER]',\n",
       "  '<emotion=sentimental> [CONTEXT] I remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world. [SEP] Was this a friend you were in love with, or just a best friend? [USER]',\n",
       "  '<emotion=sentimental> [CONTEXT] I remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world. [SEP] Was this a friend you were in love with, or just a best friend? [SEP] This was a best friend. I miss her. [USER]',\n",
       "  '<emotion=sentimental> [CONTEXT] I remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world. [SEP] Was this a friend you were in love with, or just a best friend? [SEP] This was a best friend. I miss her. [SEP] Where has she gone? [USER]',\n",
       "  '<emotion=sentimental> [CONTEXT] Was this a friend you were in love with, or just a best friend? [SEP] This was a best friend. I miss her. [SEP] Where has she gone? [SEP] We no longer talk. [USER]'],\n",
       " 'response': ['Was this a friend you were in love with, or just a best friend?',\n",
       "  'This was a best friend. I miss her.',\n",
       "  'Where has she gone?',\n",
       "  'We no longer talk.',\n",
       "  'Oh was this something that happened because of an argument?'],\n",
       " 'emotion': ['sentimental',\n",
       "  'sentimental',\n",
       "  'sentimental',\n",
       "  'sentimental',\n",
       "  'sentimental']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_processed['train'][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a871e374-6809-4492-a96a-38bec29873fd",
   "metadata": {},
   "source": [
    "### Loading the model\n",
    "- model - Blenderbot\n",
    "- input tokenize max len - 128\n",
    "- response tokenize max len - 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b90a9c44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./blenderbot_local/config.json\n",
      "Model config BlenderbotConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"BlenderbotForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1280,\n",
      "  \"decoder_attention_heads\": 32,\n",
      "  \"decoder_ffn_dim\": 5120,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": true,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 32,\n",
      "  \"encoder_ffn_dim\": 5120,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 2,\n",
      "  \"encoder_no_repeat_ngram_size\": 3,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_layer_norm\": false,\n",
      "  \"extra_pos_embeddings\": 0,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layernorm_variant\": \"prelayernorm\",\n",
      "  \"length_penalty\": null,\n",
      "  \"max_length\": null,\n",
      "  \"max_position_embeddings\": 128,\n",
      "  \"min_length\": null,\n",
      "  \"model_type\": \"blenderbot\",\n",
      "  \"no_repeat_ngram_size\": null,\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": null,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"unk_token_id\": 3,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8008\n",
      "}\n",
      "\n",
      "loading weights file ./blenderbot_local/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BlenderbotForConditionalGeneration.\n",
      "\n",
      "All the weights of BlenderbotForConditionalGeneration were initialized from the model checkpoint at ./blenderbot_local.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BlenderbotForConditionalGeneration for predictions without further training.\n",
      "loading file ./blenderbot_local/vocab.json\n",
      "loading file ./blenderbot_local/merges.txt\n",
      "loading file ./blenderbot_local/tokenizer_config.json\n",
      "loading file ./blenderbot_local/added_tokens.json\n",
      "loading file ./blenderbot_local/special_tokens_map.json\n",
      "Adding <mask> to the vocabulary\n"
     ]
    }
   ],
   "source": [
    "model = BlenderbotForConditionalGeneration.from_pretrained(\"./blenderbot_local\")\n",
    "tokenizer = BlenderbotTokenizer.from_pretrained(\"./blenderbot_local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1ea1a54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding <emotion=sad> to the vocabulary\n",
      "Adding <emotion=jealous> to the vocabulary\n",
      "Adding <emotion=devastated> to the vocabulary\n",
      "Adding <emotion=ashamed> to the vocabulary\n",
      "Adding <emotion=confident> to the vocabulary\n",
      "Adding <emotion=embarrassed> to the vocabulary\n",
      "Adding <emotion=content> to the vocabulary\n",
      "Adding <emotion=hopeful> to the vocabulary\n",
      "Adding <emotion=anticipating> to the vocabulary\n",
      "Adding <emotion=furious> to the vocabulary\n",
      "Adding <emotion=sentimental> to the vocabulary\n",
      "Adding <emotion=annoyed> to the vocabulary\n",
      "Adding <emotion=proud> to the vocabulary\n",
      "Adding <emotion=surprised> to the vocabulary\n",
      "Adding <emotion=trusting> to the vocabulary\n",
      "Adding <emotion=grateful> to the vocabulary\n",
      "Adding <emotion=disgusted> to the vocabulary\n",
      "Adding <emotion=afraid> to the vocabulary\n",
      "Adding <emotion=lonely> to the vocabulary\n",
      "Adding <emotion=faithful> to the vocabulary\n",
      "Adding <emotion=angry> to the vocabulary\n",
      "Adding <emotion=nostalgic> to the vocabulary\n",
      "Adding <emotion=joyful> to the vocabulary\n",
      "Adding <emotion=excited> to the vocabulary\n",
      "Adding <emotion=terrified> to the vocabulary\n",
      "Adding <emotion=guilty> to the vocabulary\n",
      "Adding <emotion=prepared> to the vocabulary\n",
      "Adding <emotion=apprehensive> to the vocabulary\n",
      "Adding <emotion=impressed> to the vocabulary\n",
      "Adding <emotion=anxious> to the vocabulary\n",
      "Adding <emotion=caring> to the vocabulary\n",
      "Adding <emotion=disappointed> to the vocabulary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(8041, 1280)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = list(set(dataset_processed[\"train\"][\"emotion\"]))\n",
    "special_tokens = [f\"<emotion={e}>\" for e in emotions]\n",
    "tokenizer.add_tokens(special_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a820067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the dataset\n",
    "def tokenize_fn(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    targets = tokenizer(\n",
    "        examples[\"response\"],\n",
    "        max_length=64,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42006d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a39c18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d44d6669",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns([\"input_text\", \"response\", \"emotion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992c5d10-72ed-4836-b2ed-99632e1e70cf",
   "metadata": {},
   "source": [
    "#### Loading from saved tokenizer on the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72fcd628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9d93cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = load_from_disk(\"./Chatbot Training/blender_tokenized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36ffc8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775b505c-5760-4f3b-b22b-224bd0cabd64",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d95619c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./blenderbot_empathetic\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=300,\n",
    "    gradient_accumulation_steps=2,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d5bfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2495a9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/transformers/4.21.1/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 58829\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 36770\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36770' max='36770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36770/36770 1:26:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.418200</td>\n",
       "      <td>0.776847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.302800</td>\n",
       "      <td>0.827595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.251100</td>\n",
       "      <td>0.875128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.204100</td>\n",
       "      <td>0.909350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.178300</td>\n",
       "      <td>0.929904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 9266\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./blenderbot_empathetic/checkpoint-7354\n",
      "Configuration saved in ./blenderbot_empathetic/checkpoint-7354/config.json\n",
      "Model weights saved in ./blenderbot_empathetic/checkpoint-7354/pytorch_model.bin\n",
      "tokenizer config file saved in ./blenderbot_empathetic/checkpoint-7354/tokenizer_config.json\n",
      "Special tokens file saved in ./blenderbot_empathetic/checkpoint-7354/special_tokens_map.json\n",
      "added tokens file saved in ./blenderbot_empathetic/checkpoint-7354/added_tokens.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9266\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./blenderbot_empathetic/checkpoint-14708\n",
      "Configuration saved in ./blenderbot_empathetic/checkpoint-14708/config.json\n",
      "Model weights saved in ./blenderbot_empathetic/checkpoint-14708/pytorch_model.bin\n",
      "tokenizer config file saved in ./blenderbot_empathetic/checkpoint-14708/tokenizer_config.json\n",
      "Special tokens file saved in ./blenderbot_empathetic/checkpoint-14708/special_tokens_map.json\n",
      "added tokens file saved in ./blenderbot_empathetic/checkpoint-14708/added_tokens.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9266\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./blenderbot_empathetic/checkpoint-22062\n",
      "Configuration saved in ./blenderbot_empathetic/checkpoint-22062/config.json\n",
      "Model weights saved in ./blenderbot_empathetic/checkpoint-22062/pytorch_model.bin\n",
      "tokenizer config file saved in ./blenderbot_empathetic/checkpoint-22062/tokenizer_config.json\n",
      "Special tokens file saved in ./blenderbot_empathetic/checkpoint-22062/special_tokens_map.json\n",
      "added tokens file saved in ./blenderbot_empathetic/checkpoint-22062/added_tokens.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9266\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./blenderbot_empathetic/checkpoint-29416\n",
      "Configuration saved in ./blenderbot_empathetic/checkpoint-29416/config.json\n",
      "Model weights saved in ./blenderbot_empathetic/checkpoint-29416/pytorch_model.bin\n",
      "tokenizer config file saved in ./blenderbot_empathetic/checkpoint-29416/tokenizer_config.json\n",
      "Special tokens file saved in ./blenderbot_empathetic/checkpoint-29416/special_tokens_map.json\n",
      "added tokens file saved in ./blenderbot_empathetic/checkpoint-29416/added_tokens.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9266\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./blenderbot_empathetic/checkpoint-36770\n",
      "Configuration saved in ./blenderbot_empathetic/checkpoint-36770/config.json\n",
      "Model weights saved in ./blenderbot_empathetic/checkpoint-36770/pytorch_model.bin\n",
      "tokenizer config file saved in ./blenderbot_empathetic/checkpoint-36770/tokenizer_config.json\n",
      "Special tokens file saved in ./blenderbot_empathetic/checkpoint-36770/special_tokens_map.json\n",
      "added tokens file saved in ./blenderbot_empathetic/checkpoint-36770/added_tokens.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./blenderbot_empathetic/checkpoint-7354 (score: 0.7768466472625732).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=36770, training_loss=0.30277411930581166, metrics={'train_runtime': 5194.0889, 'train_samples_per_second': 56.631, 'train_steps_per_second': 7.079, 'total_flos': 8.00205363707904e+16, 'train_loss': 0.30277411930581166, 'epoch': 5.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "691f42b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('unable to open database file')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "# trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e5386a-75ca-4d30-ab69-68aefe58d910",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "59c828ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./blender_empathetic_final\n",
      "Configuration saved in ./blender_empathetic_final/config.json\n",
      "Model weights saved in ./blender_empathetic_final/pytorch_model.bin\n",
      "tokenizer config file saved in ./blender_empathetic_final/tokenizer_config.json\n",
      "Special tokens file saved in ./blender_empathetic_final/special_tokens_map.json\n",
      "added tokens file saved in ./blender_empathetic_final/added_tokens.json\n",
      "tokenizer config file saved in ./blender_empathetic_final/tokenizer_config.json\n",
      "Special tokens file saved in ./blender_empathetic_final/special_tokens_map.json\n",
      "added tokens file saved in ./blender_empathetic_final/added_tokens.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./blender_empathetic_final/tokenizer_config.json',\n",
       " './blender_empathetic_final/special_tokens_map.json',\n",
       " './blender_empathetic_final/vocab.json',\n",
       " './blender_empathetic_final/merges.txt',\n",
       " './blender_empathetic_final/added_tokens.json')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./blender_empathetic_final\")\n",
    "tokenizer.save_pretrained(\"./blender_empathetic_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "764baf36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path = \"./Chatbot Training/blender_empathetic_final/\"\n",
    "tokenizer_final = BlenderbotTokenizer.from_pretrained(model_path)\n",
    "model_final = BlenderbotForConditionalGeneration.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf65da3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "670d80d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlenderbotForConditionalGeneration(\n",
       "  (model): BlenderbotModel(\n",
       "    (shared): BlenderbotScaledWordEmbedding(8041, 1280, padding_idx=0)\n",
       "    (encoder): BlenderbotEncoder(\n",
       "      (embed_tokens): BlenderbotScaledWordEmbedding(8041, 1280, padding_idx=0)\n",
       "      (embed_positions): BlenderbotLearnedPositionalEmbedding(128, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x BlenderbotEncoderLayer(\n",
       "          (self_attn): BlenderbotAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BlenderbotDecoder(\n",
       "      (embed_tokens): BlenderbotScaledWordEmbedding(8041, 1280, padding_idx=0)\n",
       "      (embed_positions): BlenderbotLearnedPositionalEmbedding(128, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BlenderbotDecoderLayer(\n",
       "          (self_attn): BlenderbotAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BlenderbotAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1280, out_features=8041, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_final.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949afcce-c610-4f08-bb56-e5de45cd71d0",
   "metadata": {},
   "source": [
    "### Loading our pre-trained Roberta large model for emotion detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30d161e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=32, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load emotion classifier for inference only\n",
    "from transformers import RobertaForSequenceClassification,RobertaTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1a) Load a pretrained emotion detector\n",
    "emo_tokenizer = RobertaTokenizer.from_pretrained(\"./rob-large-emotion-detector_dedupe/\")\n",
    "emo_model     = RobertaForSequenceClassification.from_pretrained(\"./rob-large-emotion-detector_dedupe/\")\n",
    "emo_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da291f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_emotion(text: str) -> str:\n",
    "    inputs = emo_tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    ).to(device)\n",
    "    \n",
    "    logits = emo_model(**inputs).logits\n",
    "    probs  = F.softmax(logits, dim=-1)\n",
    "    idx    = probs.argmax(dim=-1).item()\n",
    "    \n",
    "    return id_to_emotion[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2599dc24-8108-4f75-95f5-20bf3bd7f722",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = [\n",
    "    'jealous', 'furious', 'disgusted', 'nostalgic', 'impressed', 'faithful',\n",
    "    'caring', 'confident', 'guilty', 'angry', 'disappointed', 'sentimental',\n",
    "    'anxious', 'annoyed', 'embarrassed', 'terrified', 'apprehensive', 'grateful',\n",
    "    'sad', 'afraid', 'ashamed', 'devastated', 'joyful', 'hopeful', 'lonely',\n",
    "    'prepared', 'trusting', 'anticipating', 'excited', 'surprised', 'content', 'proud'\n",
    "]\n",
    "\n",
    "emotion_to_id = {emotion: idx for idx, emotion in enumerate(emotions)}\n",
    "id_to_emotion = {idx: emotion for emotion, idx in emotion_to_id.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104e862b-b90f-4dc2-b041-e7e9fefdcdf1",
   "metadata": {},
   "source": [
    "#### Using a pretrained model to detect distress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73b03e62-bc69-4683-addf-dbbde172ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "  from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "  import torch\n",
    "\n",
    "  tox_tokenizer = AutoTokenizer.from_pretrained(\"sentinet/suicidality\")\n",
    "  tox_model = AutoModelForSequenceClassification.from_pretrained(\"sentinet/suicidality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c80feb7-de83-407e-92a1-1c4ad5246e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# Responses\n",
    "MILD_RESPONSES = [\n",
    "    \"üíô It sounds like you're going through a tough time. You're not alone.\",\n",
    "    \"ü´∂ I'm really sorry you're feeling this way. Please know that help is available.\",\n",
    "    \"üåª You matter. Please reach out to someone you trust or a professional.\",\n",
    "    \"üå∏ I'm here for you. Talking to a counselor can really help in moments like these.\"\n",
    "]\n",
    "\n",
    "EXTREME_RESPONSES = [\n",
    "    \"üö® I'm deeply concerned about your safety. Please talk to a mental health professional or call a crisis hotline immediately.\",\n",
    "    \"‚ö†Ô∏è It sounds like you're in a lot of pain. I'm not a crisis service, but you're not alone ‚Äî please reach out to a counselor or crisis line now.\",\n",
    "    \"‚õëÔ∏è I'm just a support tool and not equipped to help in a crisis. Please talk to a licensed mental health professional right away.\"\n",
    "]\n",
    "\n",
    "HIGH_SEVERITY_KEYWORDS = {\"kill myself\", \"end of me\", \"want to die\", \"suicide\", \"die\", \"can't go on\", \"ending it all\", \"not worth living\"}\n",
    "\n",
    "\n",
    "# Safety detection function\n",
    "def detect_distress_and_severity(text, model, tokenizer, model_threshold=0.7):\n",
    "    # Model-based distress detection\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probs = torch.softmax(outputs.logits, dim=1)\n",
    "    distress_score = probs[0][1].item()\n",
    "\n",
    "    # Heuristic severity detection\n",
    "    text_lower = text.lower()\n",
    "    high_severity_flag = any(phrase in text_lower for phrase in HIGH_SEVERITY_KEYWORDS)\n",
    "\n",
    "    is_distressed = distress_score > model_threshold\n",
    "    severity = \"extreme\" if high_severity_flag else \"mild\" if is_distressed else \"none\"\n",
    "\n",
    "    return severity\n",
    "\n",
    "# # safe fallback response\n",
    "# def safe_fallback_response():\n",
    "#     return random.choice(SAFE_RESPONSES)\n",
    "\n",
    "def safety_response_handler(text):\n",
    "    severity = detect_distress_and_severity(text, tox_model, tox_tokenizer)\n",
    "\n",
    "    if severity == \"extreme\":\n",
    "        return random.choice(EXTREME_RESPONSES)\n",
    "    elif severity == \"mild\":\n",
    "        return random.choice(MILD_RESPONSES)\n",
    "    else:\n",
    "        return None  # safe to proceed with normal response generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318932b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "\n",
    "def adjust_response(response, emotion):\n",
    "    \n",
    "    emotion_responses = {\n",
    "        'furious': [\"üò° That's infuriating! \", \"üí¢ This is unacceptable! \"],\n",
    "        'proud': [\"üèÜ Incredible achievement! \", \"üëè You should be proud! \"],\n",
    "        'nostalgic': [\"üï∞Ô∏è Reminiscing can be powerful. \", \"üìª Those memories matter. \"],\n",
    "        'jealous': [\"üíö It's natural to feel this way. \", \"ü§¢ Jealousy is tough. \"],\n",
    "        'anticipating': [\"‚è≥ The wait must be intense. \", \"üîÆ Exciting things ahead! \"],\n",
    "        'sentimental': [\"üìú Those feelings are valid. \", \"üíå Heartfelt moments. \"],\n",
    "        'grateful': [\"üôè Gratitude changes everything. \", \"üåà Appreciation is beautiful. \"],\n",
    "        'caring': [\"üíñ Your compassion shines. \", \"ü§ó Kindness matters. \"],\n",
    "        'hopeful': [\"üåü Hope fuels progress. \", \"üî≠ Looking forward with you. \"],\n",
    "        'devastated': [\"üíî This is heartbreaking. \", \"üïØÔ∏è I'm here in this pain. \"],\n",
    "        'terrified': [\"üò± That sounds terrifying! \", \"üõ°Ô∏è Let's find safety. \"],\n",
    "        'ashamed': [\"üòû These feelings are valid. \", \"üõë You're safe here. \"],\n",
    "    }\n",
    "\n",
    "    # Default fallbacks\n",
    "    default_prefixes = {\n",
    "        'positive': \"üòä \",\n",
    "        'negative': \"üòü \",\n",
    "        'neutral': \"ü§ñ \"\n",
    "    }\n",
    "    \n",
    "    # We get prefix based on emotion, falling back to the appropriate default\n",
    "    if emotion in emotion_responses:\n",
    "        prefix = random.choice(emotion_responses[emotion])\n",
    "    else:\n",
    "        # We Choose default prefix based on emotion type\n",
    "        if emotion in ['joyful', 'excited', 'confident']:\n",
    "            prefix = default_prefixes['positive']\n",
    "        elif emotion in ['sad', 'anxious', 'guilty']:\n",
    "            prefix = default_prefixes['negative']\n",
    "        else:\n",
    "            prefix = default_prefixes['neutral']\n",
    "    \n",
    "    return f\"{prefix}{response}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec7e3b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5546af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    conversation_turns = []\n",
    "    print(\"Empathetic Chatbot (type 'exit' to quit)\")\n",
    "\n",
    "    while True:\n",
    "        prompt = input(\"\\nYou: \").strip()\n",
    "        if prompt.lower() == 'exit':\n",
    "            print(\"\\nBot: Goodbye! Take care. üòä\")\n",
    "            break\n",
    "        \n",
    "        response = safety_response_handler(prompt)\n",
    "        if response:\n",
    "            print(f\"Bot: {response}\")\n",
    "            continue\n",
    "    \n",
    "        # Automatically detect emotion\n",
    "        emotion = detect_emotion(user_input)\n",
    "        print(f\"‚Üí Detected emotion: {emotion}\")\n",
    "\n",
    "        # Building conversation history\n",
    "        conversation_history = \" \".join(conversation_turns)\n",
    "        if conversation_history:\n",
    "            input_text = f\"<emotion={emotion}> [CONTEXT] {conversation_history}\"\n",
    "        else:\n",
    "            input_text = f\"<emotion={emotion}> [USER] {prompt}\"\n",
    "\n",
    "        inputs = tokenizer_final(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512,\n",
    "            truncation=True\n",
    "        ).to(device)\n",
    "\n",
    "        input_len = inputs[\"input_ids\"].shape[1]\n",
    "        if input_len > 500:\n",
    "            print(f\"‚ö†Ô∏è  Warning: input length {input_len} is very close to max limit.\")\n",
    "\n",
    "        outputs = model_final.generate(\n",
    "            **inputs,\n",
    "#             max_length=128,\n",
    "            max_new_tokens=64,\n",
    "            temperature=0.9,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=2.0,\n",
    "            num_beams=4,\n",
    "            do_sample=True,\n",
    "            no_repeat_ngram_size=2,\n",
    "            length_penalty=0.9,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "        response = tokenizer_final.decode(outputs[0], skip_special_tokens=True)\n",
    "        adjusted_response = adjust_response(response, emotion)\n",
    "\n",
    "        # Update context\n",
    "        conversation_turns.append(f\"[USER] {prompt} [BOT] {adjusted_response}\")\n",
    "        if len(conversation_turns) > 3:\n",
    "            conversation_turns = conversation_turns[-3:]\n",
    "        print(f\"\\nBot: {adjusted_response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2f572f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empathetic Chatbot (type 'exit' to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  I feel really lonely you know. It's been difficult\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üí Detected emotion: afraid\n",
      "\n",
      "Bot: ü§ñ  I'm sorry to hear that. What's going on?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  It's been really difficult to do things alone. I feel anxious\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üí Detected emotion: afraid\n",
      "\n",
      "Bot: ü§ñ  I have no one to hang out with and my friends ditched me.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  yes exactly. That's what I feel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üí Detected emotion: afraid\n",
      "\n",
      "Bot: ü§ñ  I'm so sorry. That sounds really tough.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  I feel I am in a very dark place. I think something is wrong with me\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: üíô It sounds like you're going through a tough time. You're not alone.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  I feel like hurting myself is the right answer. I guess it will make me feel something\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: üíô It sounds like you're going through a tough time. You're not alone.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  I really think killing myself is the answer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: üíô It sounds like you're going through a tough time. You're not alone.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  I always think about ending it all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: üö® I'm deeply concerned about your safety. Please talk to a mental health professional or call a crisis hotline immediately.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  I think this life is not worth living.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: ‚õëÔ∏è I'm just a support tool and not equipped to help in a crisis. Please talk to a licensed mental health professional right away.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bot: Goodbye! Take care. üòä\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-env)",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
